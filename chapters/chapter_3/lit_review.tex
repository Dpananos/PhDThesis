\chapter{Literature Review}

This chapter reviews the historical development of three areas of research central to the thesis: Bayesian inference methods in personalized medicine, dynamic treatment regimes using PK/PD models, and methodology for “pooling” or data aggregation across studies. For each area, it also identifies the most current relevant work and research gaps which are addressed by this thesis.



\section{Bayesian Inference Methods in Personalized Medicine}

Statistical applications of HMC began with Neal's work on neural network models in 1996 \cite{Neal1996-vn, brooks2011handbook}, though the method was initially used to model systems of idealized molecules \cite{duane1987hybrid}. The method saw some statistical applications thereafter and appearances in statistical textbooks (as an example, Irshwaran wrote about the use of HMC to fit Bayesian generalized linear models with canonical link as well as feed forward neural networks in 1999 \cite{ishwaran1999applications}, MacKay \cite[Chapter~30]{mackay2003information} and Bishop \cite[Chapter~11]{Bishop2006pattern} included the method in their 2003 and 2006 books respectively though Bishop included the technique under its original title of Hybrid Monte Carlo, and Shcmidt used HMC in pursuit of a non-parametric Bayesian approach to non-linear regression via function factorization \cite{schmidt2009function}). Neal noted the technique still seemed to be ``under appreciated'' by statisticians in his 2011 review of the method in \textit{The Handbook of Markov Chain Monte Carlo} \cite[Chapter~5]{brooks2011handbook}, suggesting HMC was not as widely used as it could have potentially been.

Work on Stan, a probabalistic programming language which sees continued use today in both academic and industrial work \cite{betancourt2014geometric}, began in 2010 \cite{stan2012} after reseachers failed to fit the types of models they wanted to fit using existing MCMC software tools like JAGS \cite{plummer2003jags} and BUGS \cite{lunn2000winbugs}.  The inability of existing samplers to converge to the appropriate posterior in a reasonable time (if they converged at all) spurred these researchers to write an implementation of HMC, and this would eventually become the Stan programming language \cite{stan2012}.  Stan's first stable release in 2012 \cite{stan2012} implemented an improved HMC sampler known as the \textit{No-U-Turn Sampler} (or NUTS for short) \cite{stan2012, hoffman2014no}, as well as additional improvements beyond those mentioned in the NUTS paper \cite{stan2012}.  The result was a centralized, open source, and efficient implementation of HMC for researchers in the broader community to use.

Between 2012 and 2014, HMC saw empirical success in academic and industrial applications  \cite{betancourt2014geometric, Carpenter2017-qf}.  At this time, the reason for HMC's success and efficiency was poorly understood until 2014 when  Betancourt et. al \cite{betancourt2014geometric} provided a rigorous grounding of the algorithm in differential geometry.  Three years later, Betancourt published a companion article \cite{Betancourt2017-ak} providing a conceptual introduction to the algorithm, making the findings of his differential geometry research more accessible. An important finding from this research is that HMC focuses its computational resources on exploring \textit{the typical set} of the posterior distribution, a region of the parameter space where the product of probability density and volume are largest \cite{Betancourt2017-ak}.  Contributions to computations of expectations  (often expressed as an integral of the product of probability density and infinitesimal volume) come primarily from the typical set, explaining why HMC is so efficient and successful; HMC focuses on exploring the regions of parameter space which matter most \cite{Betancourt2017-ak}.  Insight into the geometric theory of HMC also explains how MAP may not be suitable as a means of summarizing the posterior for all models (especially models with many parameters).  MAP seeks the mode of the posterior distribution, (i.e. the region where posterior probability density is largest).  The intuition underlying MAP is that integration is a linear operation, and so since expectations are integrals involving probability density, then the integral and hence the expectation are primarily influenced in regions where probability density is largest \cite{Betancourt2017-ak}.  However, there exists more volume away from mode in high dimensional space than in a  neighbourhood around it.  Consequently, regions of high density do not contribute as much to computations of expectations because the volume in these regions is too small \cite{Betancourt2017-ak}.  In summary, regions around the mode contribute negligibly to expectations and methods which prioritize the mode can fail to capture the posterior with sufficient fidelity.

Despite these findings, MAP has remained a popular method for performing Bayesian inference, and has seen continued use in the pharmacokinetic and personalized medicine literature as recently as 2022 \cite{gibert2022development}.  Brooks and colleagues published a review article in 2016 on the use of Bayesian estimation for tacrolimus exposure \cite{Brooks2016-li}.  They identified 14 studies which used MAP to estimate the AUC as an indicator of tacrolimus exposure \cite{Brooks2016-li}.  Sturkenboom used MAP to derive a limited sample strategy for rifampacin, an anti-tuberculosis drug, in 2015 \cite{sturkenboom2021population, sturkenboom2015pharmacokinetic}. Kruizinga et. al used MAP to fit a population pharmacokinetic model of clonazepam from saliva and plasma samples \cite{kruizinga2022population}.  Lastly, Gibert et. al used MAP to model apixaban concentrations, providing an online tool for estimating patient decay times \cite{gibert2022development}.  

Population pharmacokinetic models like the ones used in the aforementioned studies can contain 3 parameters or more (typically clearance, elimination rate, and absorption rate, depending on the model used) and can often contain random effects for each patient for each parameter.  Assuming $N$ patients are used, the dimensionality of the model used scales like $3N$.  In the aforementioned studies, the number of patients ranges from 9 \cite{gibert2022development}to 55 \cite{sturkenboom2015pharmacokinetic}, meaning the resulting parameter space has upwards of 150 dimensions.  At this dimensionality, differences between MAP and HMC become salient. While MAP can be a good approximation to the posterior distribution for some models, it remains to be seen to what extent MAP offers an appropriate approximation to the posterior of population pharmacokinetic models, in both a predictive and decision making context.  If a Laplace approximation is made, the resulting estimates of parameter uncertainty may differ between MAP and HMC due to the assumption of constant curvature in the log posterior.  Because decision making in a Bayesian framework integrates over uncertainty, this has the potential to appreciably effect the decisions made from Bayesian pharmacokinetic models in personalized medicine.

\section{Dynamic Treatment Regimes for Decisions Motivated by Pharmacokinetics}

Much of the recent work on DTRs focuses on estimation of the value function and use of Q-learning or similar techniques for estimation of the optimal policy. Chen et. al \cite{chen2016personalized} extended an application of O-learning from Zhao et. al \cite{zhao2012estimating} to the case with continuous treatment values (i.e. doses), applying their technique to Warfarin (an anti-thrombotic drug similar to apixaban).  Laber and Zhao \cite{laber2015tree} proposed an approach to estimating optimal personalized treatment rules using decision trees, placing emphasis on their interpretability.  They applied their approach to data on a major depressive disorder to illustrate its effectiveness. Li et.al \cite{li2020utility} estimate individualized dose rules by directly balancing risks of toxicity and outcomes using a large number of biomarkers and patient covariates and an $\ell_1$ penalty.  They applied their method to patients on radiation chemotherapy for lung cancer. Park et. al \cite{park2021single} demonstrate a semi-parametric approach to estimating the dose by covariate interaction effects on treatment response and demonstrate this approach on Warfarin dose outcomes with clinical and pharmacogenetic data.  Rich et. al \cite{rich2014simulating} design a sequential multiple assignment randomized trial for optimal dose selection, and demonstrate their design by simulating data from a pharmacokinetic model based on Warfarin. They contrast Q-learning and G-estimation for estimating the parameters of various value functions which incorporate various different sources of information into the individualization process. 

Though the aformentioned studies focus on sequential decisions for drug dosing, they do not explicity use drug concentrations as their outcomes. In particular, the studies concerning applications to warfarin focus on the International Normalized Ratio (INR) as an the outcome. Warfarin is known to have narrow therapeutic window \needscite as well as drug and food interaction \needscite. Determination of a maintenance dose is consequently a procedure with frequent monitoring and followup \needscite, with some sources recommending monitoring daily or every other day until the INR stabilizes for two days \needscite. The narrow therapeutic window forces investigators to also consider the pharmacodynamics of Warfarin in addition to the pharmacokinetics when determining dose size as concentration of the drug alone is not sufficient to infer the antithrombotic effect in patients \needscite. The relationship between dose, concentrations, and pharaconyamic effect can be difficult to expressly relate, thus prompting researchers to estimate the value function (based on INR) using sufficently flexible regression method. However, because the value function must be estimated, there is a risk of model misspecification.

The introduction of factor Xa inhibitors, like apixaban, has alleviated some of the difficulties in prescribing anticoagulants. Factor Xa inhibitors have been shown to have lower risk for bleeding than Warfarin in patients withatrial fibrillation \needscite and that the plasma concentration of the drug closely correlates with its clotting effect \cite{Byon2019-gf}, making pharmacokinetic modelling more informative on antithrombotic effect as compared to Warfarin. However, a 2019 study at Western \cite{sukumar2019apixaban} discovered that there was more variation in apixaban concentrations than what had been reported in clinical trials.  These findings raised questions as to the optimal dosing of apixaban for patients in different settings.

Because apixaban concentrations correlate closely with anti-thrombotic effect, there is an opportunity to use concentration in computation of the reward for a DTR.  This would alleviate the need to estimate the reward function, thereby alleviating concerns of model mispecification.  The attention would then turn to the model for estimating concentrations, which could be supported by domain expertise from clinical pharmacologists.



\section{On Combining Pharmacokinetic Data and Pooling Information}