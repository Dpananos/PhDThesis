\chapter{Literature Review}

\section{Modern Bayesian Sampling Techniques in Personalized Medicine}

While techniques to obtain samples from a Bayesian model have existed and evolved over time, the most substantive work on efficient sampling has occurred within the last 11 years.  The use of Hamiltonian Monte Carlo (HMC) in applied statistics was initially recognized by Neal in the 1990s \cite{Neal1996-vn}, but entered the main stream in 2011 \cite{neal2011mcmc}.  In 2012, Stan (an open source C++ program to perform Bayesian inference) released their 1.0 version, implementing an adaptive variant of HMC \cite{neal2011mcmc} as well a the no-U-turn sampler \cite{hoffman2014no} which eliminated the need for the user to specify the number of steps the sampler would take in its random walk.  The release of Stan 1.0 resulted in a stable, open source toolkit for performing the most efficient and cutting edge techniques for Bayesian inference.  It is only recently that HMC's effiency has been theoretically understood.  In 2014, Betancourt et. al published \textit{The Geometric Foundations of Hamiltonian Monte Carlo} \cite{betancourt2017geometric} which grounded HMC in differential geometry, a field of pure mathematics to which statisticians are seldom exposed.

An important result from this research is that the solutions to the differential equations comprising HMC explore the \textit{typical set} of the posterior distribution \cite{Betancourt2017-ak}.  In the typical set, the product of probability density and volume is largest, and hence contributes the most to computations of expectations (which are often expressed as integrals of probability density over volumes in parameter space).  That HMC focuses on regions of parameter space which contribute the most to expectations is the reason why it is so efficient; HMC wastes no time exploring regions where the product of density and volume is small.  Insight into the geometric theory governing HMC also explains how MAP may not be suitable as a means of summarizing the posterior for all models (especially models with many parameters).  MAP seeks the mode of the posterior -- the region where posterior probability density is highest -- but as Betancourt and colleagues explain, maximum posterior density is not important, the product of density and volume is important.  In high dimensional space, more volume exists away from the mode than in a neighbourhood around it, making the mode a poor summary of the posterior, should the mode exist at all.  Additionally, the Laplace approximation which typically accompanies MAP estimation makes the assumption that the curvature of the log posterior distribution is locally constant.  This assumption can be quite fragile, especially for hierarchical models typically used in population pharmacokinetics.

Despite these findings, Maximum A Posteriori has remained a popular method for performing Bayesian inference, and has seen continued use in the pharmacokinetic and personalized medicine literature as recently as 2020. Brooks et. al \cite{Brooks2016-li} published a review which identified 14 population pharmacokinetic studies which assessed predictive performance of MAP Bayesian estimates of area under the curve (AUC) for tacrolimus  \cite{Brooks2016-li}.  Nguyen et. al used MAP estimates from a Bayesian model to derive phenotyping indexes for use in limited sampling strategies, noting the approach could reduce the time patients spend in hospital waiting to be phenotyped \cite{Nguyen2016-pg}. Preijers et. al use MAP estimation to calculate  pharmacokinetic parameters for a patient undergoing total knee replacement surgery, and use those pharmacokientic parameters to determine a dose required to obtained prescribed factor VIII target levels \cite{Preijers2019-k}.  Finally, Stifft et. al compare predictive performance of a linear regression model with a MAP estimates for a population pharmacokinetic model on tacrolimus trough levels \cite{Stifft2020-uq}.

A one compartment pharmacokinetic model can have three or four pharmacokientic parameters, each of which could potentially have an associated random effect.  The dimensionality of the resulting parameter space for these models can grow very quickly even with a modest number of patients, making differences between MAP and HMC salient.  While MAP can be a good approximation to the posterior distribution for some models, it remains to be seen to what extent MAP offers an appropriate approximation to the posterior of population pharmacokinetic models, in both a predictive and decision making context.


\section{Sequential Decision Making When Outcomes are Closely Related to Pharmacokinetics}


\textbf{Individualized Dose Rules} Much recent work on individualized dose rules (also sometimes called personalized dosing rules, or personalized treatment rules, or personalized treatment decisions) focuses on estimation of the value function and use of Q-learning or similar techniques for estimation of the optimal policy. Chen et. al \cite{chen2016personalized} extended an application of O-learning from Zhao et. al \cite{zhao2012estimating} to the case with continuous treatment values (i.e. doses).  Laber and Zhao \cite{laber2015tree} proposed an approach to estimating optimal personalized treatment rules using decision trees, placing emphasis on their interpretability. Li et.al \cite{li2020utility} estimate individualized dose rules by directly balancing risks of efficacy and outcomes using a large number of biomarkers and patient covariates and an $\ell_1$ penalty. Park et. al \cite{park2021single} demonstrate a semi-parametric approach to estimating the dose by covariate interaction effects on treatment response and demonstrate this approach on Warfarin dose outcomes with clinical and pharmacogenetic data.  Rich et. al \cite{rich2014simulating} design a sequential multiple assignment randomized trial for optimal dose selection, and demonstrate their design by simulating data from a pharmacokinetic model based on Warfarin. They contrast Q-learning and G-estimation for estimating the parameters of various value functions which incorporate various different sources of information into the individualization process.

In many of these studies, the value function must be estimated, often with a suitably flexible regression method.  Because the value function must be estimated, there is a risk of model misspecification.  In our work, we need not estimate the value function because it is a function of latent concentrations.  We integrate our uncertainty over these latent concentrations and pass them to our value function directly.  This avoids possible misspecification of the value function, allowing investigators to focus their modelling on the pharmacokinetics.
