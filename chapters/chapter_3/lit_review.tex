\chapter{Literature Review}

\section{Modern Bayesian Sampling Techniques and Their Use in Personalized Medicine}

Statistical applications of HMC began with Neal's work on neural network models in 1996 \cite{Neal1996-vn, brooks2011handbook}, though the method was initially used to model systems of idealized molecules \cite{duane1987hybrid}. The method saw some statistical applications thereafter and appearances in statistical textbooks (as an example, Irshwaran wrote about the use of HMC to fit Bayesian generalized linear models with canonical link as well as feed forward neural networks in 1999 \cite{ishwaran1999applications}, MacKay \cite[Chapter~30]{mackay2003information} and Bishop \cite[Chapter~11]{Bishop2006pattern} included the method in their 2003 and 2006 books respectively though Bishop included the technique under its original title of Hybrid Monte Carlo, and Shcmidt used HMC in pursuit of a non-parametric Bayesian approach to non-linear regression via function factorization \cite{schmidt2009function}). Neal noted the technique still seemed to be ``under appreciated'' by statisticians in his 2011 review of the method in \textit{The Handbook of Markov Chain Monte Carlo} \cite[Chapter~5]{brooks2011handbook}, suggesting HMC was not as widely used as it could have potentially been.

Work on Stan, a probabalistic programming language which sees continued use today in both academic and industrial work \needscite, began in 2010 \cite{stan2012} after reseachers failed to fit the types of models they wanted to fit using existing MCMC software tools like JAGS \cite{plummer2003jags} and BUGS \cite{lunn2000winbugs}.  The inability of existing samplers to converge to the appropriate posterior in a reasonable time (if they converged at all) spurred these researchers to write an implementation of HMC, and this would eventually become the Stan programming language \cite{stan2012}.  Stan's first stable release in 2012 \cite{stan2012} implemented an improved HMC sampler known as the \textit{No-U-Turn Sampler} (or NUTS for short) \cite{stan2012, hoffman2014no}, as well as additional improvements beyond those mentioned in the NUTS paper \cite{stan2012}.  The result was a centralized, open source, and efficient implementation of HMC for researchers in the broader community to use.

Between 2012 and 2014, HMC saw empirical success in academic and industrial applications  \cite{betancourt2014geometric, Carpenter2017-qf}.  At this time, the reason for HMC's success and efficiency was poorly understood until 2014 when  Betancourt et. al \cite{betancourt2014geometric} provided a rigorous grounding of the algorithm in differential geometry.  Three years later, Betancourt published a companion article \cite{Betancourt2017-ak} providing a conceptual introduction to the algorithm, making the findings of his differential geometry research more accessible.

An important finding from this research is that HMC focuses its computational resources on exploring \textit{the typical set} of the posterior distribution, a region of the parameter space where the product of probability density and volume are largest \cite{Betancourt2017-ak}.  Contributions to computations of expectations  (often expressed as an integral of the product of probability density and infinitesimal volume) come primarily from the typical set, explaining why HMC is so efficient and successful; HMC explores where it matters \cite{Betancourt2017-ak}.  Insight into the geometric theory of HMC also explains how MAP may not be suitable as a means of summarizing the posterior for all models (especially models with many parameters).  MAP seeks the mode of the posterior distribution -- the region where posterior probability density is largest.  The intuition underlying map is that integration is a linear operation, and so since expectations are integrals involving probability density, then the integral and hence the expectation are primarily influenced in regions where probability density is largest \cite{Betancourt2017-ak}.  However, there exists more volume away from mode in high dimensional space than in a  neighbourhood around it.  Consequently, regions of high density do not contribute as much to computations of expectations because the volume in these regions is too small \cite{Betancourt2017-ak}.  In summary, regions around the mode contribute negligibly to expectations and methods which prioritize the mode can fail to capture the posterior with sufficient fidelity.

Despite these findings, MAP has remained a popular method for performing Bayesian inference, and has seen continued use in the pharmacokinetic and personalized medicine literature as recently as 2021 \needscite.

Population pharmacokinetic models like the ones used in the aforementioned studies \needscite can contain 3 parameters or more (typically clearance, elimination rate, and absorption rate, depending on the model used) and can often contain random effects for each patient for each parameter.  Assuming $N$ patients are used, the dimensionality of the model used scales like $3N$.  In the aforementioned studies, the number of patients ranges from x to y \needscite, meaning the resulting parameter space has upwards of xy dimensions.  At this dimensionality, differences between MAP and HMC become salient. While MAP can be a good approximation to the posterior distribution for some models, it remains to be seen to what extent MAP offers an appropriate approximation to the posterior of population pharmacokinetic models, in both a predictive and decision making context.  If a Laplace approximation is made, the resulting estimates of parameter uncertainty may differ between MAP and HMC due to the assumption of constant curvature in the log posterior.  Because decision making in a Bayesian framework integrates over uncertainty, this has the potential to appreciably effect the decisions made from Bayesian pharmacokinetic models in personalized medicine.

\section{Dynamic Treatment Regimes for Decisions Motivated by Pharmacokinetics}

\section{On Combining Pharmacokinetic Data and Pooling Information}