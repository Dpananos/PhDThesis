\chapter{Literature Review}

\section{Modern Bayesian Sampling Techniques and Their Use in Personalized Medicine}

MCMC techniques to sample from distributions have evolved over time, with notable algorithms including The Metropolis algorithm \cite{metropolis1953equation},  Gibbs sampling \cite{geman1984stochastic}, and what would eventually come to be known as Hamiltonian Monte Carlo (initially called Hybrid Monte Carlo, retaining the abbreviation HMC) \cite{duane1987hybrid}.  Statistical applications of HMC began with Neal's work on neural network models in 1996 \cite{Neal1996-vn, brooks2011handbook}. The method saw some statistical applications thereafter and appearances in statistical textbooks (as an example, Irshwaran wrote about the use of HMC to fit Bayesian generalized linear models with canonical link as well as feed forward neural networks in 1999 \cite{ishwaran1999applications}, MacKay \cite[Chapter~30]{mackay2003information} and Bishop \cite[Chapter~11]{Bishop2006pattern} included the method in their 2003 and 2006 books respectively,  though Bishop included the technique under its original title of Hybrid Monte Carlo, and Shcmidt used HMC in pursuit of a non-parametric Bayesian approach to non-linear regression via function factorization \cite{schmidt2009function}) but Neal noted the technique still seemed to be ``under appreciated'' by statisticians in his 2011 review of the method in \textit{The Handbook of Markov Chain Monte Carlo} \cite[Chapter~5]{brooks2011handbook}.

Sometime in 2010, several researchers (including Andrew Gelman, Wei Wang, Vince Dorie, Ben Goodrich, Matt Hoffman, Michael Malecki, Bob Carpenter and Daniel Lee) sought out to apply full Bayesian inference to multilevel generalized linear models, which included such complexities as: hierarchical covariance priors, non-conjugate coefficient priors, latent effects, and varying output link functions \cite{stan2012}.  They noted ``[t]he models we wanted to fit turned out to be a challenge for current general purpose software to fit. A direct encoding in BUGS \cite{lunn2000winbugs} or JAGS \cite{plummer2003jags} can grind these tools to a
halt'' (\cite{stan2012}, citations added).  The inability of existing samplers to converge to the appropriate posterior in a reasonable time (if they converged at all) spurred these researchers to write an implementation of HMC, and this would eventually become the Stan programming language \cite{stan2012}.  Stan's first stable release in 2012 \cite{stan2012} implemented an improved HMC sampler known as the \textit{No-U-Turn Sampler} (or NUTs for short) \cite{hoffman2014no}, as well as additional improvements beyond those mentioned in the NUTS paper \cite{stan2012}.  The result was a centralized, open source, and efficient implementation of HMC for researchers in the broader community to use.

Between 2012 and 2014, HMC has empirical success in academic and industrial applications  \cite{betancourt2014geometric, Carpenter2017-qf}.  At this time, the reason for HMC's success and efficiency was poorly understood until 2014 when  Betancourt et. al \cite{betancourt2014geometric} provided a rigorous grounding of the algorithm in differential geometry.  Later, Betancourt published a companion article in 2017 \cite{Betancourt2017-ak} providing a conceptual introduction to the algorithm, making the findings of his differential geometry research more accessible.

An important finding from this research is that HMC focuses its computational resources on exploring \textit{the typical set} of the posterior distribution, a region of the parameter space where the product of probability density and volume are largest \cite{Betancourt2017-ak}.  Contributions to computations of expectations  (often expressed as an integral of the product of probability density and infinitesimal volume) come primarily from the typical set, explaining why HMC is so efficient and successful; HMC explores where it matters \cite{Betancourt2017-ak}.  Insight into the geometric theory of HMC also explains how MAP may not be suitable as a means of summarizing the posterior for all models (especially models with many parameters).  MAP seeks the mode of the posterior distribution -- the region where posterior probability density is largest.  The intuition underlying map is that integration is a linear operation, and so since expectations are integrals involving probability density, then the integral and hence the expectation are primarily influenced in regions where probability density is largest.  However, there exists more volume away from mode in high dimensional space than in a  neighbourhood around it.  Consequently, regions of high density do not contribute as much to computations of expectations because the volume in these regions is too small \cite{Betancourt2017-ak}.  In summary, regions around the mode contribute negligibly to expectations and methods which prioritize the mode can fail to capture the posterior with sufficient fidelity to be useful.

Despite these findings, MAP has remained a popular method for performing Bayesian inference, and has seen continued use in the pharmacokinetic and personalized medicine literature as recently as 2021. Brooks et. al \cite{Brooks2016-li} published a review in 2016 which identified 14 population pharmacokinetic studies which assessed predictive performance of MAP Bayesian estimates of area under the curve (AUC) for tacrolimus. Luque et. al published a population pharmacokinetic mdodel of anidulafungin in critically ill patients in 2019 using MAP \cite{luque2019population}.  In 2020, Stifft et. al compare predictive performance of a linear regression model with a MAP estimates for a population pharmacokinetic model on tacrolimus trough levels \cite{Stifft2020-uq}.  Gao et. al published a population pharmacokinetic model in 2021 for high-dose methotrexate in Chinese pediatric patients using MAP \cite{gao2021population}. 

Population pharmacokinetic models like the ones used in the aformentioned studies \cite{luque2019population, Stifft2020-uq, gao2021population} contain 3 parameters or more (typically clearance, elimination rate, and absorption rate, depending on the model used) and can often contain random effects for each patient and each parameter.  Assuming $N$ patients are used, the dimensionality of the model used scales like $3N$.  In the aforementioned studies, the number of patients ranges from 27 \cite{Stifft2020-uq} to 311 \cite{gao2021population}, meaning the resulting parameter space has upwards of 60 dimensions.  At this dimensionality, differences between MAP and HMC become salient. While MAP can be a good approximation to the posterior distribution for some models, it remains to be seen to what extent MAP offers an appropriate approximation to the posterior of population pharmacokinetic models, in both a predictive and decision making context.  