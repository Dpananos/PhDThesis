\chapter{Literature Review}

This chapter reviews the historical development of three areas of research central to the thesis: Sampling methods for Bayesian models, dynamic treatment regimes using PK/PD models, and methodology for “pooling” or data aggregation across studies. For each area, it also identifies the most current relevant work and research gaps which are addressed by this thesis.



\section{Bayesian Inference Methods in Personalized Medicine}

Statistical applications of HMC began with Neal's work on neural network models in 1996 \cite{Neal1996-vn, brooks2011handbook}, though the method was initially used to model systems of idealized molecules \cite{duane1987hybrid}. The method saw some statistical applications thereafter and appearances in statistical textbooks (as an example, Irshwaran wrote about the use of HMC to fit Bayesian generalized linear models with canonical link as well as feed forward neural networks in 1999 \cite{ishwaran1999applications}, MacKay \cite[Chapter~30]{mackay2003information} and Bishop \cite[Chapter~11]{Bishop2006pattern} included the method in their 2003 and 2006 books respectively though Bishop included the technique under its original title of Hybrid Monte Carlo, and Schmidt used HMC in pursuit of a non-parametric Bayesian approach to non-linear regression via function factorization \cite{schmidt2009function}). Neal noted the technique still seemed to be ``under appreciated'' by statisticians in his 2011 review of the method in \textit{The Handbook of Markov Chain Monte Carlo} \cite[Chapter~5]{brooks2011handbook}, suggesting HMC was not as widely used as it could have potentially been.

Work on Stan, a probabilistic programming language which sees continued use today in both academic and industrial work \cite{betancourt2014geometric}, began in 2010 \cite{stan2012} after researchers failed to fit the types of models they wanted to fit using existing MCMC software tools like JAGS \cite{plummer2003jags} and BUGS \cite{lunn2000winbugs}.  The inability of existing samplers to converge to the appropriate posterior in a reasonable time (if they converged at all) spurred these researchers to write an implementation of HMC, and this would eventually become the Stan programming language \cite{stan2012}.  Stan's first stable release in 2012 \cite{stan2012} implemented an improved HMC sampler known as the \textit{No-U-Turn Sampler} (or NUTS for short) \cite{stan2012, hoffman2014no}, as well as additional improvements beyond those mentioned in the NUTS paper \cite{stan2012}.  The result was a centralized, open source, and efficient implementation of HMC for researchers in the broader community to use.

Between 2012 and 2014, HMC saw empirical success in academic and industrial applications  \cite{betancourt2014geometric, Carpenter2017-qf}.  At this time, the reason for HMC's success and efficiency was poorly understood until 2014 when  Betancourt et al.\ \cite{betancourt2014geometric} provided a rigorous grounding of the algorithm in differential geometry.  Three years later, Betancourt published a companion article \cite{Betancourt2017-ak} providing a conceptual introduction to the algorithm, making the findings of his differential geometry research more accessible. An important finding from this research is that HMC focuses its computational resources on exploring \textit{the typical set} of the posterior distribution, a region of the parameter space where the product of probability density and volume are largest \cite{Betancourt2017-ak}.  Contributions to computations of expectations  (often expressed as an integral of the product of probability density and infinitesimal volume) come primarily from the typical set, explaining why HMC is so efficient and successful; HMC focuses on exploring the regions of parameter space which matter most \cite{Betancourt2017-ak}.  Insight into the geometric theory of HMC also explains how MAP may not be suitable as a means of summarizing the posterior for all models (especially models with many parameters).  MAP seeks the mode of the posterior distribution, (i.e. the region where posterior probability density is largest).  The intuition underlying MAP is that integration is a linear operation, and so since expectations are integrals involving probability density, then the integral and hence the expectation are primarily influenced in regions where probability density is largest \cite{Betancourt2017-ak}.  However, there exists more volume away from mode in high dimensional space than in a  neighbourhood around it.  Consequently, regions of high density do not contribute as much to computations of expectations because the volume in these regions is too small \cite{Betancourt2017-ak}.  In summary, regions around the mode contribute negligibly to expectations and methods which prioritize the mode can fail to capture the posterior with sufficient fidelity.

Despite these findings, MAP has remained a popular method for performing Bayesian inference, and has seen continued use in the pharmacokinetic and personalized medicine literature as recently as 2022 \cite{gibert2022development}.  Brooks and colleagues published a review article in 2016 on the use of Bayesian estimation for tacrolimus exposure \cite{Brooks2016-li}.  They identified 14 studies which used MAP to estimate the AUC as an indicator of tacrolimus exposure \cite{Brooks2016-li}.  Sturkenboom used MAP to derive a limited sample strategy for rifampicin, an anti-tuberculosis drug, in 2015 \cite{sturkenboom2021population, sturkenboom2015pharmacokinetic}. Kruizinga et al.\ used MAP to fit a population pharmacokinetic model of clonazepam from saliva and plasma samples \cite{kruizinga2022population}.  Lastly, Gibert et al.\ used MAP to model apixaban concentrations, providing an online tool for estimating patient decay times \cite{gibert2022development}.  

Population pharmacokinetic models like the ones used in the aforementioned studies can contain 3 parameters or more (typically clearance, elimination rate, and absorption rate, depending on the model used) and can often contain random effects for each patient for each parameter.  Assuming $N$ patients are used, the dimensionality of the model used scales like $3N$.  In the aforementioned studies, the number of patients ranges from 9 \cite{gibert2022development} to 55 \cite{sturkenboom2015pharmacokinetic}, meaning the resulting parameter space has upwards of 150 dimensions.  At this dimensionality, differences between MAP and HMC become salient. While MAP can be a good approximation to the posterior distribution for some models, it remains to be seen to what extent MAP offers an appropriate approximation to the posterior of population pharmacokinetic models, in both a predictive and decision making context.  If a Laplace approximation is made, the resulting estimates of parameter uncertainty may differ between MAP and HMC due to the assumption of constant curvature in the log posterior.  Because decision making in a Bayesian framework integrates over uncertainty, this has the potential to appreciably effect the decisions made from Bayesian pharmacokinetic models in personalized medicine.

\section{Dynamic Treatment Regimes for Decisions Motivated by Pharmacokinetics}

Much of the recent work on DTRs focuses on estimation of the value function and use of Q-learning or similar techniques for estimation of the optimal policy. Chen et al.\ \cite{chen2016personalized} extended an application of O-learning from Zhao et al.\ \cite{zhao2012estimating} to the case with continuous treatment values (i.e. doses), applying their technique to Warfarin (an anti-thrombotic drug similar to apixaban).  Laber and Zhao \cite{laber2015tree} proposed an approach to estimating optimal personalized treatment rules using decision trees, placing emphasis on their interpretability.  They applied their approach to data on a major depressive disorder to illustrate its effectiveness. Li et al.\ \cite{li2020utility} estimate individualized dose rules by directly balancing risks of toxicity and outcomes using a large number of biomarkers and patient covariates and an $\ell_1$ penalty.  They applied their method to patients on radiation chemotherapy for lung cancer. Park et al.\ \cite{park2021single} demonstrate a semi-parametric approach to estimating the dose by covariate interaction effects on treatment response and demonstrate this approach on Warfarin dose outcomes with clinical and pharmacogenetic data.  Rich et al.\ \cite{rich2014simulating} design a sequential multiple assignment randomized trial for optimal dose selection, and demonstrate their design by simulating data from a pharmacokinetic model based on Warfarin. They contrast Q-learning and G-estimation for estimating the parameters of various value functions which incorporate various different sources of information into the individualization process. 

Though the aforementioned studies focus on sequential decisions for drug dosing, they do not explicity use drug concentrations as their outcomes. In particular, the studies concerning applications to Warfarin focus on the International Normalized Ratio (INR), a measure of blood thinness, as an the outcome. Warfarin is known to have narrow therapeutic window \cite{merli2009warfarin} as well as drug and food interaction \cite{juurlink2007drug}. Determination of a maintenance dose is consequently a procedure with frequent monitoring and followup \cite{carris2015feasibility}, with some sources recommending monitoring for up to 12 weeks with stable INR \cite{holbrook2012evidence}. The narrow therapeutic window forces investigators to also consider the pharmacodynamics of Warfarin in addition to the pharmacokinetics when determining dose size as concentration of the drug alone is not sufficient to infer the antithrombotic effect in patients. The relationship between dose, concentrations, and pharaconyamic effect can be difficult to expressly relate, thus prompting researchers to estimate the value function (based on INR) using sufficently flexible regression method. However, because the value function must be estimated, there is a risk of model misspecification.

The introduction of factor Xa (read as ``factor ten A'') inhibitors, like apixaban, has alleviated some of the difficulties in prescribing anticoagulants. Factor Xa inhibitors have been shown to have lower risk for bleeding than Warfarin in patients with atrial fibrillation \cite{touma2015meta} and that the plasma concentration of the drug closely correlates with its thinning effect \cite{Byon2019-gf}, making pharmacokinetic modelling more informative on anti-thrombotic effect as compared to Warfarin. However, a 2019 study at Western \cite{sukumar2019apixaban} discovered that there was more variation in apixaban concentrations than what had been reported in clinical trials.  These findings raised questions as to the optimal dosing of apixaban for patients in different settings.

Because apixaban concentrations correlate closely with anti-thrombotic effect, there is an opportunity to use concentration in computation of the reward for a DTR.  This would alleviate the need to estimate the reward function, thereby alleviating concerns of model misspecification.  The attention would then turn to the model for estimating concentrations, which could be supported by domain expertise from clinical pharmacologists.

\section{Combining Pharmacokinetic Data and Pooling\\ Information}

Personalized medicine seeks to understand and explain heterogeneity in response and safety to drugs.  Hence, there may be the need to tailor models for a specific population.  To that effect, research on how clinical and genetic factors affect drug concentrations and adverse events is increasingly happening in academic medical centres  \cite{gibert_development_2022, gulilat_association_2022, peretz_pharmacokinetics_2021, gulilat_drug_2020, sturkenboom_population_2021}.  Many of these studies use proprietary software like NONMEM \cite{bauer2011nonmem} to fit their models, closely emulating research into pharmacokinetics done by drug companies \cite{cirincione2018population,ueshima2018population}.  

Those studies which do not use NONMEM or similar software may resort to linear models \cite{gulilat2020drug}.  Though linear models are a strong and popular tool for applied statistical modelling, they suffer from multiple drawbacks for pharmacokinetic research, including misspecification of the pharmacokientic function, under determination of the effects of covariates, and inability to incorporate uncertainty from one pharmacokientic parameter into another (for example, uncertainty in the time to max concentration into estimates of the max concentration).  Additionally, the studies into pharmacokinetic modelling often use variable selection methods (including fitting all submodels \cite{cirincione2018population,ueshima2018population}).  There are known deficiencies to variable selection approaches including but not limited to: bias away from the null \cite{whittingham2006we}, exaggerated precision \cite{altman1989bootstrap}, inaccurate or uninterpretable $p-$values due to inability to properly incorporate uncertainty in the selection process \cite{harrell2015regression}, and failure to select the ``true" model with high confidence even when modelling assumptions are consistent with the true data generating process \cite{smith2018step}.  Variable selection is intended to answer ``which variables are important to modelling the outcome", and as a consequence those variables which are not selected are considered as ``negligible."  In the Bayesian setting, different methods are used for this purpose.  In particular, Bayesian models with sparsity inducing priors have been used as a means of selecting relevant predictors of clinical outcomes, especially in the genome wide association literature \cite{ni2019bayesian, armero2019two, zhou2013polygenic}.  In these approaches, effects are regularized towards a null effect, with a few variables having appreciable impact and the remaining having negligible impact. The resulting estimates are biased towards the null, but in taking on this additional bias the resulting estimates have lower variance.  For some studies, especially exploratory studies where the goal is to discover new avenues for future research, this trade off may be acceptable.

Additionally, academic medical centres may not have access to the same resources a drug company would, limiting their ability to run clinical studies with tightly controlled conditions.  Consequently, drug concentration data can come from both tightly controlled studies \cite{tirona2018apixaban} and observational studies such as those from a personalized medicine clinic \cite{sukumar2019apixaban, gulilat2020drug}. The tightly controlled studies often can sample subjects multiple times from a single dose (as well as on an empty stomach, eliminating any possible effects of diet) facilitating estimation of between subject variation, while the observational studies often collect a single sample from patients. Combining these two types of data would be the most efficient use for academic researchers, and existing research \cite{cirincione2018population} has combined these types of data for estimation of pharmacokientic parameters.  Because data coming from observational studies only have a single observation, estimation of between subject variation is precluded, meaning between subject variation manifests as increased residual variance.  This difference in residual variance between observational and controlled clinical studies means patients can not be treated as exchangeable. Exchangeability is closely related to the IID assumption, and treating data as exchangeable when they are not is similar to treating data as IID when they are not.  The result is exaggerated precision in resulting estimates and possible attenuation/exaggeration of effects.  Since these estimates are used to make decisions, extreme outcomes are less probable due to the exaggerated precision and extreme outcomes are hence estimated to be less likely than they are in reality.

The increase in pharmacokinetic research at academic centres for personalized medicine stands to benefit from models which can properly pool information between studies, avoid problems associated with variable selection, and are written in open source languages.