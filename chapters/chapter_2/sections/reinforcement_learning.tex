\section{Dynamic Treatment Regimes and Q Learning}



In the following two subsections, I present background material on dynamic treatment regimes, which are used to develop optimal decision-making models.

\subsection{Dynamic Treatment Regimes}


A dynamic treatment regime (DTR) is a sequence of decision rules for adapting a treatment plan to the time-varying state of an individual subject \cite{chakraborty2013statistical}. In DTRs, and their cousin topic in computer science \textit{reinforcement learning}, an agent (often thought of as a robot in reinforcement learning, but within medicine sometimes thought of as a physician’s computerized decision support system) interacts with a system a number of times. In the terms of DTRs and reinforcement learning, each interaction with the system is considered a \textit{stage}.  At each stage, the agent receives an \textit{observation} of the system and then determines an \textit{action} to take. This action will result in an observed \textit{reward} which is followed by a new observation of the system after it has been impacted by the action.  This cycle of observation, action, reward then repeats, with the agent aiming to take actions which yield the largest total reward. For more on reinforcement learning and DTRs, see \cite{lizotte17reinforcement, chakraborty2013statistical}.

\subsection{Trajectories}

The data generated by the cycle of observation, action, and reward from the initial action to the final reward is called a \textit{trajectory}. Formally, we define a stage to be a triple containing an observation, chosen action, and resulting reward. Let $O_i$ denote an observation at the $i^{th}$ stage, $ A_i $ be the action at the $ i^{th} $ stage, and $ Y_i $ denote the reward at the $ i^{th}$ stage,  in capital letters when considering the observation, action, and reward as random variables. Following notation by Chakraborty and Moodie \cite{chakraborty2013statistical},  define the history of the system at stage $j$ to be $ H_j = (O_1, A_1, O_2, A_2, \cdots , O_{j-1}, A_{j-1}, O_j) $.  The reward at stage $j$ can be thought of as a function of the system’s history, the action taken, and possibly the new state of the system $ Y_j = Y_j(H_j, A_j, O_{j+1}) $.  As we explain in the next section, the expected sum of rewards from each stage under different actions is of primary interest in DTRs.  Since the reward is a random variable, the sum of rewards is also a random variable.  We refer to the expectation of the sum of rewards as \textit{the value}, and we refer to the observed sum of rewards as \textit{the return}. Importantly, rewards reflect the immediate desirability of single action, where as value reflects longer term desirability of a sequence of actions.

\subsection{Policies, Value Functions, and Q-Learning}

Let $K$ be the number of stages in a DTR.  A policy $ d = (d_1, \cdots, d_K) $ is a vector of decision rules each of which take as input the system’s history and output an action to take.  Each decision rule is a function $d_j : \mathcal{H}_j \to \mathcal{A}_j$ where $\mathcal{H}_j$ and $\mathcal{A}_j$ are the history and action spaces at stage $j$ respectively.  The stage $ j $ value function for a decision rule $ d $ is the expected sum of rewards the agent would receive starting from history $ h_j  $ (here in lower case since it is an observed quantity) if it chose actions according to $ d $ for every action thereafter.  The stage $j$ value function is 
\begin{equation}
	V^d_j(h_j) = E_d\left[ \sum_{k=j}^K Y_k(H_k, A_k, O_{k+1}) \Bigg\lvert H_j = h_j\right] \>.
\end{equation}

\noindent Here, the expectation is over the distribution of trajectories. Since the value is expected the sum of rewards, the stage $ j $ 
value function can be decomposed into the expectation of reward at stage $ j $ plus the stage $ j+1  $ value function  \cite{chakraborty2013statistical}
\begin{equation}
	V^d_j(h_j) = E_d\left[Y_j(H_j, A_j, O_{j+1}) + V^d_{j+1}(H_{j+1}) \vert H_j = h_j\right] \>.
\end{equation}


\noindent The optimal stage $ j  $ value function is the value function under a policy which yields maximal value

\begin{equation}
	V^{opt}_j(h_j) = \max_{d \in \mathcal{D}} \left\{ V^d_j(h_j) \right\} \>.
\end{equation}

\noindent  Here, $\mathcal{D}$ is the space of policies.  Estimating a policy that maximizes value can be achieved by estimating the optimal Q function \cite{chakraborty2013statistical}.  The optimal Q function at stage $ j $ is a function of the system’s history $ h_j $ and a proposed action $ a_j $,
\begin{equation}
	Q_j^{opt}(h_j, a_j) = E \left[ 
	Y_j(H_j, A_j, O_{j+1}) + V^{opt}_{j+1}(H_{j+1}) \lvert H_j = h_j, A_j = a_j
	\right].
\end{equation}

\noindent Note that the optimal Q function has similar form and interpretation to the optimal value function (namely, it is the expected return \textemdash the value \textemdash starting at stage $ j $ with history $h_j$ but with the added condition that we take action $ a_j $ now and then follow the optimal policy thereafter). 

Given the optimal Q function, an optimal policy is given by 
\begin{equation}
	d_j^{opt}(h_j) = \arg\max_{a\in \mathcal{A}} \left\{Q_j^{opt}(h_j,a)\right\} \>.
\end{equation}


\subsection{Similarity to Statistical Decision Theory}

Dynamic treatment regimes and reinforcement learning concern learning a policy to obtain maximal value.  Thus, they are concerned with multi-stage decision making under uncertainty.  These frameworks bear a resemblance to statistical decision theory, in which a single decision is to be made under uncertainty. Following \cite{berger2013statistical}, there exists an unknown quantity or quantities $\boldsymbol{\theta} \in \boldsymbol{\Theta}$ called \textit{the state of nature} which affects the decision process and  which may require estimation using data, $\mathbf{X}$ .  Associated with every state of nature and decision (more commonly called an \textit{action}), $a$, is an associated loss incurred, $\mathcal{L}(\boldsymbol{\theta}, a)$.  From a Bayesian perspective, the goal is then to determine the action, $a^{opt}$ which minimizes the Bayesian expected loss 


\begin{align}
	a^{opt} &=  \arg\min_{a \in \mathcal{A}} \left\{ 	E^{\pi}\left[ \mathcal{L}(\boldsymbol{\theta},a) \right] \right\} \\
	E^{\pi}\left[ \mathcal{L}(\boldsymbol{\theta},a) \right] &= \int_{\boldsymbol{\Theta}} \mathcal{L}(\boldsymbol{\theta}, a)  \pi(\theta) \, d\theta\label{reimann_stieltjes}
\end{align}

\noindent Here $\pi$ is the believed probability distribution of $\boldsymbol{\theta}$ at the time of decision making.  If data and a model are available, then $\pi$ could be the posterior distribution of $\boldsymbol{\theta}$ after conditioning the model on data.  Similar approaches exist when using a Frequentist perspective, but they will not be discussed here because this thesis is primarily concerned with Bayesian models.  For more information on Frequentist approaches to decision making under uncertainty see \cite{berger2013statistical}.  Assuming a Bayesian perspective again, minimizing the expected Bayesian loss in statistical decision theory is equivalent to minimizing the negative reward in a single stage DTR.  