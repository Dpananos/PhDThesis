\section{Bayesian Statistics}

In this section, I introduce some key concepts of Bayesian statistics to be used in the remainder of the thesis.  I begin by briefly reviewing how Bayesianism differs from Frequentism in philosophy.  I then introduce Bayes nets as a tool for visualizing the factorization of the joint distribution.  Finally, I discuss model diagnostics and MCMC computation for Bayesian inference.


\subsection{The Bayesian Philosophy on Probability}
%\todo[inline]{This is a can of worms. Re-frame this paragraph by saying what specific other people say about this issue and citing. "So-and-so characterises frequentism as (cite). "Gelman charcterises the Bayesian approach as (cite). Jaynes is right out." Or, "Many researchers describe Bayesian approach as subjective (citecitecite)." }


Gelman and colleagues have described Frequentist interpretations of probability as the `` relative frequency obtained in a long sequence of [events], assumed to be performed in an identical manner, physically independent of one another ''   \cite[p.~12]{gelman2013bayesian}.   Frequentism has been contrasted against Bayesianism, which has sometimes been said to view probability as a subjective form of uncertainty quantification \cite{gelman2013bayesian}.

Core to Bayesian statistics is Bayes' rule \cite[p.~7]{gelman2013bayesian}

\begin{equation}\label{Bayes}
	p( \bm{\theta} \vert y) \propto p(y \vert \bm{\theta}) p(\bm{\theta}) \>.
\end{equation}

\noindent  Here, $ \bm{\theta} $  and $y$ can refer to events (e.g. $p(\bm{\theta} \vert y)$ could refer to the probability of having a disease given you test positive), but in the remainder of this thesis $\bm{\theta}$ will refer to model parameters, and $ y $ to observed data.  The distributions $ p( \bm{\theta} \vert y) $, $ p(y \vert \bm{\theta}) $, and $p(\bm{\theta})$ are referred to as the \textit{posterior}, \textit{the likelihood}, and \textit{the prior} respectively.  Since the product of Bayes' rule is a probability distribution (i.e. the probability of the parameters conditioned on the data), inferences resulting from a Bayesian analysis are expressed in probabilistic statements. Bayesian modelling begins by specifying a full probability model for the phenomenon.  A likelihood for the data generating process is specified, and prior knowledge about the parameters is codified in terms of a probability distribution (i.e.\ the prior).  Conditioning on the observed data is performed, and the posterior is calculated and interpreted.  Finally, the resulting model is evaluated and the implications of the resulting posterior are assessed.  

\subsection{Bayesian Networks}



Bayesian models describe the full joint distribution over all variables and parameters of interest, and can become quite complex. Bayesian networks (Bayes nets) can be used to simplify model description by dividing the model into components and then describing the relationships between those components. A Bayes net is a directed acyclic graph which represents a factorization of the joint probability distribution of the model. The nodes of the graph denote random variables (which include parameters), while the edges denote dependence of the child node on the parent node \cite{Bishop2006pattern}. 

Shown in \cref{bayesnet} is an example of a Bayes net for Gelman's rat tumour example in \cite{gelman2013bayesian}.  A total of $ N = 71 $ experiments on lab rats have been conducted in the past to assess the risk of developing endometrial stromyl polyps.  A rat can either develop the endometrial stromal polyp, or not develop the endometrial stromyl polyp, so the number of rats which develop the polyp, $ y_i $, is modelled as binomial.  Each of the 71 previous experiments is modelled as having it's own risk of developing the polyp, $ \theta_i $, which we postulate are drawn from a beta distribution with parameters $ \alpha $ and $ \beta $.  

Algebraically, the model would be written as 
%
\begin{align*}
	[\alpha,\beta]^T &\sim p(\alpha, \beta) \\
	\theta_i \vert \alpha, \beta &\sim \operatorname{Beta}(\alpha,\beta) \quad i = 1 \dots N\\
	y_i \vert \theta_i, \alpha, \beta &\sim \operatorname{Binomial}(\theta_i ; n_i) \quad i = 1 \dots N
	\end{align*}
%
Here, $ p(\alpha, \beta)  $ is the prior for the parameters of the beta distribution, and $ n_i $ is the number of rats in the $ i^{th} $ experiment.  This same model is written as a Bayes net in \cref{bayesnet}.   The node containing $ \alpha $ and $ \beta $ is the parent of $ \theta $, indicating that $ \theta $ depends on $ \alpha $ and $ \beta $, and $ y $ is the child of $ \theta $, indicating that $ y $ depends on $ \theta $. It is worth noting that if the parent nodes are known, then the child nodes are independent of all other nodes in the model. The rectangle surrounding $ \theta $ and $ y $ signifies that there are $ N $ such copies of these random variables which are considered exchangeable conditioned on the parent nodes.  Instead of explicitly writing out all $ N=71 $ of these random variables, we instead place them in what is known as a ``plate", and indicate in the bottom corner how many replicates there are.  Bayes nets make it very easy to write out the posterior distribution of the parameters.  We simply follow the net from the bottom up, conditioning each node on it's parent nodes.  The posterior for this model is then

 \[ p(\alpha,\beta, \theta \vert y) \propto p(\alpha,\beta) \prod_{i = 1}^N p(\theta_i \vert \alpha, \beta) p(y_i\vert \theta_i)  \]

\begin{figure}[h!]

	\centering
	\begin{tikzpicture}
		\node[latent] (ab) {$\alpha,\beta$};
		\node[latent, below = of ab]  (theta) {$\theta$};
		\node[obs, below = of theta](y){$y$};
		
		\edge{ab}{theta};
		\edge{theta}{y};
		
		\plate{exp}{(theta)(y)}{$N$};
	\end{tikzpicture}
	\caption[Hierarchical binomial model bayes net]{Bayes net for hierarchical binomial model.  Note here that nodes with shading correspond to observed data, while nodes without shading are latent, or unobserved.}
	\label{bayesnet}
\end{figure}

\noindent Note that while the Bayes net describes the dependence structure of the model, it does not specify the forms of the individual conditional distributions; these must also be provided to fully specify the model.

\subsection{Model Assessment}

Once a model is specified, and the posterior for the parameters obtained, the model fit, not only to the data but also to the practitioner's substantive knowledge, must be assessed.  

Since the result of a Bayesian analysis is the posterior distribution over the model parameters given the observed data, data can be simulated from the inferred data generating process.  Let $ y $ be observed data, and $ \theta $ be a vector of parameters for the model.  Denote $ \tilde{y} $ as replicated data from the data generating process, or as Gelman writes, ``data that we \textit{would} have seen tomorrow if the experiment that generated $ y $ today were replicated with the same model and the same value of $ \theta $ that produced the observed data" \cite[page~145]{gelman2013bayesian}.  Then the distribution of the replicated data conditioned on the observed data is 
%
\begin{equation}\label{PPD}
	p(\tilde{y} \vert y) = \int p(\tilde{y} \vert \bm{\theta}) p(\bm{\theta} \vert y) \, d\bm{\theta}  \>.
\end{equation}
%
The distribution in \cref{PPD} is called the \textit{posterior predictive distribution}.  If the model fits the data well, then observed data should look plausible under the posterior predictive distribution. In a posterior predictive check, simulated data sets are generated from \cref{PPD} and are compared to the observed data.  Any systematic differences between observed and simulated may point to areas in which the model can be improved.

\subsection{Maximum A Posteriori (MAP) and The Laplace Approximation to the Posterior}

The integrals in Bayesian statistics required to evaluate probabilities quickly become intractable even when considering simple models. Point estimates for model parameters can be obtained by maximizing the log posterior density.  If the posterior is $p(\bm{\theta} \vert y)$, then a point estimate for $\bm{\theta}$ is

\begin{align}
	\bm{\theta}_{\text{MAP}} &= \underset{\bm{\theta} \in \mathbb{R}^p}{\arg\max} \Big\{ \log p(\bm{\theta} \vert y) \Big\} \nonumber \\ 
	& = \underset{\bm{\theta} \in \mathbb{R}^p}{\arg\max} \Big\{\log p(y \vert \bm{\theta}) + \log p(\bm{\theta}) \Big\}  \nonumber
\end{align}

\noindent This approach is known as \textit{Maximum A Posteriori} or MAP for short \cite{murphy2012machine}.  The point estimate  $\bm{\theta}_{\text{MAP}} $ corresponds to the mode of the posterior distribution.  If the prior $p(\bm{\theta})$ places uniform probability over all values of $\bm{\theta}$ then the MAP estimate corresponds to the maximum likelihood estimate.  When the prior  is not uniform over $\bm{\theta}$, then the prior acts as a regularizing term on the maximum likelihood estimate, with more informative priors offering more regularization.

The MAP estimate is attractive because of its speed (as compared to other forms of estimation for Bayesian models discussed below).  However, it is only a point estimate rather than a distribution.  To further approximate the posterior distribution, a Laplace Approximation to the posterior can be made by performing a quadratic approximation to the log posterior density \cite{murphy2012machine}.  The result of this technique is that the posterior is locally modelled as Gaussian

$$ p(\bm{\theta} \vert y)  \approx \Normal(\bm{\theta}_{\text{MAP}}, \Lambda^{-1}) \>. $$

\noindent Here, $\Lambda$ is the estimated precision matrix, obtained by computing the Hessian of the negative log posterior at the MAP estimate.


\subsection{Markov Chain Monte Carlo \& Modern Methods}

Computational methods have been developed that can sample from the posterior distribution without having to know the exact analytical form of the posterior distribution. The suit of computational methods for sampling from the posterior are called \textit{Markov Chain Monte Carlo} (MCMC) methods.  These methods simulate Markov chains whose limiting distribution is the posterior distribution \cite{livingstone2016geometric}.  The earliest MCMC methods for drawing  samples from the posterior are The Metropolis-Hastings Algorithm and Gibbs Sampling \cite{gelman2013bayesian,mcelreath2016statistical}.  Recently however, these algorithms have given way to more efficient algorithms, known as \textit{Hamiltonian Monte Carlo} (HMC) methods.  In these methods, the posterior is idealized as a high dimensional surface in the model's parameter space on which a particle of mass $ m $ rolls after being given a random position and momentum.  The geometry of the surface influences the movement of the particle, and thus influences the samples obtained.   The theory for HMC is quite dense and so  I refer interested readers to the following resources \cite{ gelman2013bayesian, livingstone2016geometric, mcelreath2016statistical,neal2011mcmc, hoffman2014no,betancourt2017conceptual} for further explanation.  Because of HMC's complexity and requisite background theory on Hamilonian dynamics and measure theory, I will consider HMC a black box for the purposes of this thesis.

All of these sampling methods result in a user specified number of sequences of samples of size $ M$, often simply referred to as \textit{chains}. After obtaining $ M $ samples, and preferably omitting the first $ K $, the remaining $ M-K $ samples are treated as \textit{iid} draws from the posterior distribution.  The chains can then be used to estimate expectations of model parameters, uncertainty in those estimates, and make predictions.

\subsection{Diagnostics for Hamiltonian Monte Carlo}

%Under certain conditions, a law of large numbers and a central limit theorem exists for these Markov chains \cite{livingstone2016geometric, betancourt2017robust}, allowing users to be confident in inferences made from the samples obtained.  General conditions under which a chain is and is not geometrically ergodic exist \cite{livingstone2016geometric}, however these properties can be assessed via diagnostics on the Markov chains themselves.

In MCMC and HMC, several chains are usually initialized and allowed to run for sufficiently long to as to (hopefully) arrive at their stationary distribution.  If geometric ergodicity holds, then all chains should arrive at the same stationary distribution, and thus be exploring the same space.  The Gelman-Rubin diagnostic  measures how well the chains are exploring the space by comparing the within chain variance to the between chain variance \cite{gelman2013bayesian}.  In practice, $ 1.05<\rhat $ indicates a problem with the chains, and inference should not be made from the samples drawn \cite{betancourt2017robust}.

Another diagnostic is the effective sample size, $ n_{\mathit{eff}} $.  Theories of convergence of functions of random variables that assume independence are inappropriate as the samples from the Markov chains are correlated.  Effective sample size is a heuristic used to measure how close the samples are to being independent.  Effective sample size is defined as
%
\[ n_{\mathit{eff}} = \dfrac{M-K}{1+ 2\displaystyle\sum_k \rho(k)} \>. \]
%
Here $ M-K $ is the length of the chain and $ \rho(k) $ is the lag-$k$ within chain correlation \cite{gelman2013bayesian,kass1998markov}.  If the chains are autocorrelated, then $n_{\mathit{eff}} <M-K$, and if the chains are completely independent $n_{\mathit{eff}} = M-K$. If chains are highly correlated, then $n_{\mathit{eff}} \ll M-K$, and inferences made from the samples should be avoided because of the bias the correlation would impart.