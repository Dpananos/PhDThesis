\section{Background}

\subsection{Apixaban}

Apixaban is a direct acting oral anti-coagulant often prescribed for prevention of stroke and systemic embolism in patients with atrial fibrillation (AF) \cite{BMSmonograph,byon2019apixaban}.  Studies as recent as 2019 have reported excess variability in observed apixaban plasma concentrations in patients with AF \cite{sukumar2019apixaban}. Since apixaban plasma concentrations correlate closely with anti-coagulation, excess variability in these concentrations may mean increased risk of bleeding. These findings have raised questions towards the optimal dosing of apixaban in older adults with AF encountered outside of clinical trials. Additional research into determining factors which explain this excess variability beyond known clinical factors \cite{gulilat2020drug} has consequently begun.

\subsection{Variable Selection}


Existing studies into pharmacokinetic modelling often use variable selection methods (e.g. variants of stepwise selection, including fitting all submodels \cite{cirincione2018population,ueshima2018population}) when faced with the determining which variables effect the pharmacokientics. Many studies have noted that these techniques result in bias away from the null \cite{whittingham2006we}, exaggerated precision \cite{altman1989bootstrap}, inaccurate or uninterpretable $p-$values due to inability to properly incorporate uncertainty in the selection process \cite{harrell2015regression}, and can fail to select the "true" model with high confidence even when modelling assumptions are consistent with the true data generating process \cite{smith2018step}.  Hence, even in the best case scenario where the selection procedure identifies the correct variables, the resulting estimates may not be reliable.  Results from these studies methods make a convincing argument to avoid selection methods all together.

Variable selection methods are intended to answer the question "which variables are important in modelling the outcome", and although studies have demonstrated deficiencies with variable selection, they often do not provide an alternative answer.  From a Bayesian perspective, selection to include a variable in or out of a model defines a sort of prior on the parameter value; there is a strong preference for a null effect estimate unless the data provide sufficient evidence for free estimation of that effect.  Efforts to operationalize this prior structure in terms of Bayesian inference have lead to a wide variety of sparsity inducing priors, which include spike and slab priors \cite{mitchell1988bayesian}, and horseshoe \cite{carvalho2010horseshoe} and Finnish horseshoe priors \cite{piironen2017sparsity}. These approaches admit that while unlikely that the effects of unimportant variables are exactly 0, they may be small enough to be negligible.  These priors place the majority of their probability mass near 0, encouraging small effects to be estimated as something negligibly small, but allow for large effects to be identified at the cost of a small amount of bias.

Bias towards a null effect  can be acceptable when the goal is exploration and prediction.  The bias can act as a regularization for predictions hence combating overfitting, and can hedge estimates of novel effects when they are exaggerated due to high variance. To this end, we present a simulation study in which we use a sparsity inducing prior to estimate the effect of a concomitant medication on apixaban pharmacokinetics.  In particular, the medication is assumed to inhibit a particular gene important in the elimination of apixaban, making the bioavailability or half-life larger.  We place a double exponential (or Laplace) prior on the effect of the concomitant medication, as well as a prior on the parameter for the Laplace distribution.  This is similar to putting a LASSO penalty on the effect as well as a prior on the LASSO penalty strength \cite{tibshirani1996regression}.  Although our simulation only has a single variable of interest, many variables can be used with this prior structure.

For our simulation, we generate data from the posterior of a previously fit model. We simulate 10 datasets from a pre-specified number of repeatedly sampled patients (we examine 5, 10, 20, 30, 40, and 50 repeatedly sampled patients) haven taken their first dose of the drug with a pre-specified and fixed effect of a concomitant medication on the bio-availability of the drug.  We assume that investigators can sparsely sample patients more easily, and so we simulate 10 times more sparsely sampled patients who have already achieved steady state. We do this so as to more closely resemble real life scenarios in which patients come into a clinic for a plasma measurement having already been on the drug for sometime. We examine effects of 0, 0.125, 0.25, 0.5, 1.0, and 1.5 on the logit scale (we use the logit scale since bioavailability is constrained to be between 0 and 1). 

\subsection{Why Is a Hierarchical Model Needed?}

In this paper, we propose a pooling of both sparsely and repeatedly sampled data in a single model. Pooling information is not a new approach, and reasonable arguments could be made to use simpler models.  After all, if the sparsely sampled data models a continuous outcome as a function of covariates, why would investigators use a complex model when something simple like linear regression (or linear regression on log concentrations) may be sufficient?  While simpler approaches and criticisms of using unnecessarily complex models are valid, both linear modelling and mixed effects models for pooling suffer from important drawbacks in the case when attempting to combine sparsely sampled and repeatedly sampled data from different studies.  We examine those drawbacks below.

Linear regression can be, and has been \cite{gulilat2020drug,vakkalagadda2016effect}, used to model apixaban concentrations as a function of time and other covariates using sparsely sampled data.  When certain criteria are met, there is good reason to do so.  The concentration profile, $C(t)$, from a first order absorption with linear elimination pharmacokinetic model looks like

$$ C(t) = \frac{F \cdot D}{C l} \frac{k_{e} \cdot k_{a}}{k_{e}-k_{a}}\left(e^{-k_{a}t}-e^{-k_{e}t}\right) $$

\noindent Here, it is usually assumed that $k_e<k_a$ in order for the model to be identified \cite{wakefield1992bayesian, salway2008gamma}. The elimination phase occurs when $t$ is sufficiently large, resulting in $C(t)$ being approximately exponential and $\log(C(t))$ being linear in time on the log scale with slope $-k_e$.  The assumption that measurement error is additive on the log scale facilitates use of linear regression.

This approach is common in pharmacokinetics when estimating the elimination rate but suffers from three important drawbacks generally. First, the elimination rate is not allowed to vary as a function of known factors which effect elimination rate, such as kidney function.  This can be ameliorated by specifying an interaction between time and those covariates known to effect elimination rate (though this has not been done in all studies \cite{gulilat2020drug}).  Second, an exponential approximation is only appropriate when time is sufficiently large.  Clearly, the exponential approximation breaks down near $t=t_{max} = \log(k_a/k_e)/(k_a-k_e)$ and is completely inappropriate in the absorption phase when $t<t_{\max}$.  This affects estimates of max concentration in an appreciable way, resulting in an upward bias of $C_{\max}$.  Additionally, because $t_{\max}$ is not modelled per individual, estimates of $C_{\max}$ must rely on a point estimate of $t_{\max}$.  This results in uncertainty estimates of $C_{\max}$ which may be too narrow for a given individual.  Finally, the effects of covariates on other aspects of the pharmacokinetcs are undetermined. Assuming a linear model is used to model concentrations on the log scale, we find

\begin{align}
	\log(C(t)) & = \log(D) + \log(F) - \log(Cl) + \log(k_e) + \log(k_a) - \log(k_e-k_a) + \log\Big( e^{-k_{a}t}-e^{-k_{e}t} \Big) \nonumber \\
	                 & \approx \log(D) + \beta_0 + \beta_1t \qquad \mbox{when  } t_{\max} \ll t \>. \nonumber
\end{align}


\noindent Here, $\beta_0 =  \log(F) - \log(Cl) + \log(k_e) + \log(k_a) - \log(k_e-k_a)$.  If covariates are included in the model, then although changes in log concentration may be accurate (in so far as the sign of the chance in log concentration is concerned), \textit{where that change occurs is under determined}.  Did concentration increase because bioavailability ($F$ in the log linear model) increased, or was it because the clearance rate ($Cl$ in the log linear model) decreased?  We can't say for certain from this model. In order to determine if a change in concentration was due to an increase/decrease in a pharmacokinetic parameter, each pharamacokinetic parameter must be modeled as functions of covariates. How salient these drawbacks are is up to the investigator, but if any of them are important to decision making for personalized medicine then a linear model will not be appropriate.

Mixed effects models can be used to pool information from many datasets.  Meta-analysis is perhaps the most prevalent example of this approach.  A typical example may be pooling data from studies conducted using similar protocols across multiple centers.  Ideally, the data are collected under similar protocols, making assumptions regarding the likelihood and exchangeability of appropriate units tenable.  In the scenario we describe, where information from at least two studies with different protocols are to be pooled, we believe a mixed effect model specifying between study variation is not appropriate due to subjects not being exchangeable between studies.  Recall, a sequence of random variables $\theta_1, \dots, \theta_n$ is said to be exchangeable in their joint density $p(\theta_1, \dots, \theta_n)$ is invariant permutations of the indicies $(1, \dots, n)$ \cite{gelman1995bayesian}.  If no other information, other than observed data, is available to distinguish any of the $\theta_j$ from any others, and no ordering or grouping of parameters can be made, one must assume exchageability of the $\theta$.  In the scenarios we describe, we do have additional information which can be used to distinguish the $\theta$.  In particular, sparsely sampled data will have a larger estimated residual error than repeatedly sampled data.  This is because the residual variance is a combination of within and between subject variation. There are then 2 residual variances to be estimated: one for the repeatedly sampled data and one for the sparsely sampled data.  When pooling sparsely sampled and repeatedly sampled data together, individuals within subjects are exchangeable because of the common residual variance within study. However, subjects are not exchangeable between studies because permutations of the subject indicies fail to account for which subject should be associated with which residual error.