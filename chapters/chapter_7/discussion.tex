\chapter{Discussion}

This thesis has provided contributions towards the goals of identifying factors driving between patient variability in drug response, and selecting the optimal dose for a patient.  These problems were approached from the context of pharmacokinetics, arguing that drivers of variability in concentrations may be drivers of variability in response since concentration is a proxy for systematic exposure.  The first article presented a comparison of a population pharmacokinetic model fit using Maximum A Posteriori and Hamiltonian Monte Carlo for the purposes of decision making on dosing.  Additionally, a one compartment pharmacokinetic model with first order elimination was presented which leveraged a non-dimensionalization to force identifiability of the model and facilitated use of Hamiltonian Monte Carlo for sampling from the posterior distribution. The simulation studies from this article provided evidence that models fit using Hamiltonian Monte Carlo resulted in better calibrated decisions when the goal is to select a dose to avoid the risk of exceeding some concentration threshold.  The second article provided a framework for combining Bayesian pharmacokinetic models with dynamic treatment regimes for the comparison of various modes of personalization.  A case study on apixaban was used demonstrate the benefits of various forms of personalization and motivated conversation on if the benefits would outweigh the burden placed on the patient to adhere to additional follow up. The final article highlighted important violations of assumptions of exchangeability when combining data from different studies and offered a model which satisfies those assumptions, allowing investigators to combine all data available to them to perform inference on pharmacokinetic models. Sparsity inducing priors were also motivated as a means of determining if novel variables have an appreciable affect on pharmacokinetic measures, such as the drug bio availability. The sparsity inducing priors approach trades off variance for a small amount of bias and side steps issues associated with variable selection, a common approach used in determining which variables to include in a pharmacokinetic model.

\section{Key Themes}

\subsection{Paper 1}
New advancements in statistical theory can take time to be adopted across disciplines. In the case of HMC, theoretical understanding is still nascent, but additional theoretical and applied evidence for preferring HMC over MAP continues to mount.  Though theoretical warnings for MAP's deficiency were published earlier this decade, the first article in this thesis demonstrated that deficiency could manifest in non-obvious ways for models important to personalized medicine.  Indeed, models fit by HMC and MAP appeared to be equivalent when compared on predictive accuracy, but decisions made therefrom were very different with different outcomes.  MAP is motivated by low dimensional intuition --- that because integrals are linear operators, and density is largest around the mode, then the areas around the mode should contribute most to expectations.  The importance of intuition in statistics, and mathematical modelling of any kind more broadly, can not be over stated.  However, that intuition can fail in spectacular ways when dimensionality increases due to the so called ``Curse of Dimensionality''.

The shift from ``low'' dimensionality where intuition is effective to ``high'' dimensionality happens quickly. Modelling intuition needs to be validated, and that validation may not necessarily come directly from examining the fitted model (e.g. by examining the distribution of residuals).  The importance of simulating fake data and fitting proposed models to that data is a known but often unreported approach to model validation.  This approach may not be necessary for all techniques (for example when models are fit via optimization and the objective function is convex with unique optima, such in the case of most generalized linear models), but for non-standard models or models which are highly non-linear, it can be an effective tool for discovering hidden modelling pathologies.  Since the publication of the first article, additional research has been published on a ``Bayesian workflow''  \cite{gelman_bayesian_2020} in which fitting models to simulated data is listed as an explicit step.  Often, the inferences we wish to make go beyond that of parameter values, and fake data simulation can help in ensuring that the resulting model is capable of returning accurate inferences, or discovering that the model as written is incapable of doing so.

Additionally, fake data simulation can provide evidence that the model may not be able to be fit as it is written.  While HMC is effective and efficient, modelling benefits do not come for free.  When models suffer from pathologies detectable from sampling diagnostics, the remedies to those pathologies can be non-trivial. A naive implementation of the model presented in the first article suffered from slow sampling times, and often chains failed to converge to the same target distribution.  Often, the key to an effective implementation comes down to an effective parameterization, and this was the case for this model.  The non-dimensionalization offered a way to force identifiability of the model, and the result was an efficient sampling with no detectable pathologies.

\subsection{Paper 2}

Sequential decision making is an important aspect of personalized medicine.  However, there are sometimes in which sequential optimization is important, and others when it makes a negligible difference.  The second article in this thesis  provided a framework for evaluating different modes of personalization.  In that article, sequentially optimal modes of personalization yielded smaller regret on average than those which only used information from a single point in time.  However, the magnitude of that difference --- at least in the case studies presented --- were small.  The decision to implement one mode of personalization over another would thus also need to consider costs (monetary or otherwise) associated with each mode. While the considerations for those decisions may vary from facility to facility, statistical methodology can help to determine how much better one mode could perform over another \textit{in ideal circumstances}.

I don't know, something something something.

\subsection{Paper 3}

The observation that some drugs have variability in concentrations in excess of that observed in clinical trials motivates the ``fine tuning'' of pharmacokinetic models to a population of interest.  However, the data resources available to pharmaceutical companies may not be realistic for individual researchers or smaller institutions to obtain.  Making use of data collected on the same drug in different studies is one way to increase the data available for personalization.  The development of new methodologies and models to facilitate this combination is crucial, as the models will likely have to account for individual study peculiarities. Those developments can not happen in isolation and will require a collaboration between modellers and domain experts.

Investigators seeking to develop such new methods must be careful not to make simplifying assumptions which may threaten the internal validity of the inference. Previous models studying the effects of various clinical factors on apixaban concentration were perhaps \textit{too simple}\footnote{I feel comfortable saying this because \textit{I} was the one who recommended the approach \cite{gulilat_drug_2020}}.  The resulting inferences may have been valid from a statistical perspective, but could lack useful interpretation in the applied domain depending on which questions are being asked of the model.  This provides an extreme example, and future research would likely be subject to more nuanced challenges, such as the violation of exchangeability discussed in the third article. Carefully navigating the modelling process will require collaboration with expert modellers.  Those modellers must also be receptive to receiving feedback from domain experts, and must work diligently to extract necessary information from those experts.


\subsection{Future Work}

The foundation of this work is the Bayesian pharmacokinetic model, hence additional effort should be put into ensuring the model is of sufficient quality and aligns with expert knowledge.  In particular, ensuring the priors reflect expert knowledge as accurately as possible is a clear avenue for improvement.  This work could take many forms, including a Bayesian meta analysis to synthesize effects from various previous studies.  Additionally, the problem of prior elicitation from experts has spurred research in human computer interaction, resulting in interactive ways of evaluating the agreement of priors with expert knowledge \cite{sarma2020prior}.  Once priors are agreed upon, the construction of a Bayesian model from the population of interest can be performed.  This thesis leveraged data from highly controlled clinical study for most modelling efforts.  A more heterogeneous sample may lead to better generalizability, and hence better decisions made from said model.  Further work could be done to refine inferences from the measurement process too.  If instruments are calibrated, using data from the calibration process can provide useful information usable by the model.

The work presented in this thesis surrounding optimal sequential decision making used a value function which was simple and easy to interpret.  In reality, the value function used implicitly in the clinic is more complex and likely varies between clinicians.  The dosing decisions made by clinicians offer the opportunity to learn the implied value function and then use that value function in a dynamic treatment regime.  Additionally, there is the opportunity to explicitly incorporate health economic factors into the  framework presented here for purposes of comparing modes of personalization.

When combining data from different studies, it may be unlikely that all studies measure the same variables.  Work on Bayesian inference for missing variables could be used to extend the work I presented in the third article.  Additionally, while the variables from well controlled studies are likely high precision, those variables obtained from studies with less precision may benefit from an ``error in variables approach''.  These approaches will undoubtedly add uncertainty to the model, but honest uncertainty is likely preferable to exaggerated precision.

\section{Conclusion}

This thesis presented techniques for identifying factors driving variation in drug response and optimal dose selection using Bayesian statistics and dynamic treatment regimes in conjunction with pharmacokinetic models.  The methodologies presented here evaluated 

