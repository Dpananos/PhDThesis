---
title: "Bayesian Pharmacokinetic Models for Inference and Optimal Sequential Decision Making"
subtitle: "with Applications in Personalized Medicine" 
fontsize: 10pt
author: "Demetri Pananos M.Math" 
date: "October 19 2022"
bibliography: "thesis_bib.bib"
output:
  beamer_presentation:
    slide_level: 3
    keep_tex: false
institute: Department of Epidemiology and Biostatistics
header-includes: 
  - \usepackage{subfig}
  - \usepackage{graphicx}
  - \usepackage{booktabs}
  - \usepackage{setspace}
  - \usepackage{xcolor}
  - \usepackage{multicol}
  - \usepackage{tikz}
  - \usepackage{bm}
  - \definecolor{uwo-purple}{HTML}{4F2683}
  - \definecolor{uwo-gray}{HTML}{807F83}
  - \usecolortheme[named=uwo-purple]{structure}
  - \setbeamertemplate{headline}{\hfill\includegraphics[width=2.5cm]{Figures/UWO_Horizontal_Full.png}\hspace{0.2cm}\vspace{-.8cm}}
  # set options for blocks 
  - \setbeamertemplate{blocks}[rounded]
  - \setbeamercolor{block title alerted}{fg=white, bg=uwo-purple} 
  # cahnge alert block colours 
  - \setbeamercolor{block body alerted}{bg=uwo-gray!25}
 # - \setbeamertemplate{footline}{\insertframenumber} # gets n/N 
  - \usepackage{threeparttablex}
  - \makeatletter
  - \setbeamertemplate{footline} { \tikz[overlay]{\node at(12,.25){\thepage};} }
  - \makeatother
  
# to get logo bottom right corner; works with Rochester theme
  # - \logo{\includegraphics[width=.1\textwidth]{./Figures/uwo-purple.png}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


### Motivation

* Different people react differently to the same drug, even if they match on important variables. This is can be an obstacle for optimal treatment:

  * Heightened response ($\to$ toxicity)
  * Lowered response ($\to$ inefficacy)
  
* Personalized medicine is a response to this problem.

### Personalized Medicine

*Personalized Medicine (PM)*: Application of genomic, demographic, and lifestyle factors as predictors of disease risk and individualization of drug therapy [@morse2015personalized].

Morse and Kim identify 4 goals of PM:

  1) Identify drugs for which excess variation in response is key issue to treatment
  2) **Identify factors driving this variation**
  3) **"Right amount, right drug, right patient, right time"**
  4) Aid in prevention of adverse events associated with said drugs

### Excess Variation Motivates Fine Tuning

* *Pharmacokinetics* (PK): Time course of drug concentrations in the body. 
  * Understand variation in concentration since drug concentration $\leftrightarrow$ drug exposure.
* Many PK studies provide dose adjustment criteria based on modelling
* Excess variation beyond that observed in clinical trials [@sukumar2019apixaban].  Motivates a "fine tuning" to population at hand.

### How to "Fine Tune"

* Bayesian statistics is one possible formalization of "fine tuning" previous modelling.
* PM is about a sequence of decisions (about treatment).  Adopt a formalization of sequential decision making (dynamic treatment regimes (DTR)).
* Decisions in PM are based on concentrations (PK).  Synergy between PK modelling and DTRs.

### This Thesis

* Methods for creating Bayesian PK models for:
  * Inference on covariate effects on concentrations, and
  * Use in optimal sequential decision making on dose size.
  
Hence, this thesis is most closely aligned with goals \textcolor{uwo-purple}{(2) and (3)} of personalized medicine.

### Objectives & Contrubutions

* Compare/contrast existing approaches to fitting Bayesian models with recent advancements in pursuit of fitting population PK models.
  * One compartment model based on non-standard parameterization.  Simulation study demonstrating MAP leads to poorer calibrated decisions.
* Develop a framework for evaluating the benefits of collecting additional information against burden put on patient.
  * Framework for development of simulation-based evaluation of personalization based on PM combined with DTRs.
* Demonstrate how PM researchers in academic centers can use all data available to them to study effects of variables on PK while also exploring new variables.
  * Demonstration of how to combine observational and tightly controlled data. Comments on and reparameterizations to maintain exchangeability.

## Paper 1: Comparisons Between HMC and MAP for a Bayesian Model for Apixiban Induction Dose and Dose Personalization

### Background

* Hamiltonian Monte Carlo (HMC) considered gold standard for sampling from a Bayesian model.  Theoretical understanding came within last 10 years.
* Prior to that, Maximum A Posteriori (MAP) a popular method for fitting Bayesian models.
* Theoretical arguments *against* using in some models, mostly based on differential geometric arguments.
* Question:
  * Are decisions in PM greatly affected by choice of inference method?
  * Answer: Predictions of concentration are similar, but uncertainty is different.  This affects decision quality.
  
### Motivating Theory

* Intuition for MAP is based on low dimensional thinking.
* Theory for HMC says MAP should break down as number of parameters in the model increases.
* For some PK models, number parameters might scale like $3N$.
* Might effect estimates of concentration, might effect uncertainty estimates too.

### Experiments

* Need a ground truth that looks similar to real data.
* Fit a model using HMC to real data and use posterior predictive to generate 100 new simulated patients.
* Refit the model on this simulated data, this time with MAP and HMC. 
* Compare decision quality based on two criteria:
  * Pick a dose so the maximum concentration is not too large (toxicity)
  * Pick a dose so the trough concentration is not too small (inefficacy)


### The Model

* 1 Compartment PK model with first order elimination and absorption.
* Prior distributions derived from a 2019 review on apixaban pharmacokinetics.
* Each patient gets their own parameters through a random effect
$$y(t)= \begin{cases}\frac{F \cdot D}{C l} \frac{k_e \cdot k_a}{k_e-k_a}\left(e^{-k_a(t-\delta)}-e^{-k_e(t-\delta)}\right) & \delta \leq t \\ 0 & \text { else }\end{cases}$$$$ \log(Cl_j) \sim \mathcal{N}(\beta_{0, Cl}, \sigma^2_{Cl}) $$$$ \log(t_{\max, j}) \sim \mathcal{N}(\beta_{0, t_{\max}}, \sigma^2_{t_{\max}}) $$$$ \operatorname{logit}(\alpha_j) \sim \mathcal{N}(\beta_{0, \alpha}, \sigma^2_\alpha) $$$$ \delta_j \sim \operatorname{Beta}(\mu \kappa \>, (1-\mu) \kappa) $$

### Modelling Results 

![[Left] Draws from the prior distribution.  [Center] Two patients from our dataset. [Right] Posterior fits for the latent concentration.](../figures/fig3.png)

### How Does It Fit?

![Diagnostics of the Bayesian model fit to real data.](../figures/diagnostics.png){width=80%}

### Checking Decision Quality

* Model fits well, now how do we check decision quality?
* Look at predictions
* Look at decisions under uncertainty
  * Pick a time, $T$ (e.g. time to max concentration)
  * Pick a concentration threshold, $C$, we would like to keep concentration under.
  * Pick a desired risk level, $R$.
  * Select a dose so that $P\Big(y(T) > C\Big) = R$

### Checking Decision Quality

* If select a dose for every patient such that they have probability $R$ of exceeding the threshold, then approximately $R\times 100$ patients should exceed the threshold.
* Check the calibration of dosing decisions using models fit by MAP and HMC.

### Prediction Error Very Similar

* Prediction error is very similar, no reason to prefer one over the other.

```{r, echo=F, message=F, warning=F}
library(tidyverse)
library(kableExtra)

tribble(
  ~"Loss", ~'HMC', ~"MAP",
  "MSE (Sd)", "6.67 (15.93)", "8.57 (19.93)",
  "MAE (Sd)", "1.71 (1.94)", "1.97 (2.17)",
  "MAPE (Sd)",  "0.04 (0.03)", "0.05 (0.03)"
) %>% 
  kbl(format = 'latex') %>% 
  kable_styling(bootstrap_options = 'striped')
```

### Calibration is Very Different

* HMC very well calibrated, MAP not so much.
* Remember, same model and same priors. Different decisions.
* Why is this happening?

![Calibration on decisions. [Left] Calibration for decision on max concentration.  [Right] Calibration on trough concentration.](../figures/fig8.png){width=80%}

### Uncertainty is Larger in MAP Model

* MAP results in excess uncertainty as compared to HMC.
* Because decisions integrate over uncertainty, differences in uncertainty $\to$ differences in decisions.

![19 of the 100 simulated patients who demonstrate excess variation.](../figures/intervals.png){width=95%}

### Discussion

* MAP leads to different decisions than HMC, even with more data than would be available in practice.
  * To the extent that this would affect decisions depends on the loss.
* MAP is fast and familiar to MLE, but it is not interchangeable with HMC for these types of models.
* Recommend that if you are to use MAP, invest the time to compare against HMC.

## Paper 2: Developing and Evaulating Pharmacokinetics-Driven Dynamic Personalized Medicine: A Framework and Case Study

### Background

* With respect to the third goal of PM (personalized dosing), many studies  regress stable dose onto pre-dose covariates (*static personalization*).

* Need for dose titration has been reduced, but not eliminated.

* Possibility to personalize the titration process too for optimal decision making (*dynamic personalization*).

* Is the juice of dynamic personalization worth the squeeze?
  * Are the possible gains from dynamic personalization worth imposing additional burden onto patients?

### Framework

* Have a PK model for a given drug, fit it on new data.
* Use the model to simulate new patients (much like before)
* Try different forms of personalization on these simulated subjects
* Because we know the truth, determine the "return" from each form of personalization.

### Model

* Similar to last model, but now adjust for patient covariates.

$$y(t)= \begin{cases}\frac{F \cdot D}{C l} \frac{k_e \cdot k_a}{k_e-k_a}\left(e^{-k_a(t-\delta)}-e^{-k_e(t-\delta)}\right) & \delta \leq t \\ 0 & \text { else }\end{cases}$$
$$ \log(Cl_j) \sim \mathcal{N}\Big(\beta^{Cl}_{0, j} + \mathbf{x}^{Cl}_j \beta_{Cl}\>, \sigma^2_{Cl}\Big)$$
$$ \log(t_{\max, j}) \sim \mathcal{N}\Big(\beta^{t_{\max}}_{0, j} + \mathbf{x}^{t_{\max}}_j \beta_{t_{\max}} \>, \sigma^2_{t_{\max}} \Big)$$
$$ \operatorname{logit}(\alpha_j) \sim \mathcal{N}\Big(\beta^{\alpha}_{0, j} + \mathbf{x}^{\alpha}_j \beta_{\alpha} \>, \sigma^2_{\alpha} \Big)$$

### Forms of Personalization

* Goal: Keep patients within some concentration range
* Not intended to be exhaustive, just demonstrative of capabilities
* Range from
  * No personalization
  * Personalize on covariates
  * Personalize on covariates + 1 measurement
  * Sequentially optimal via Q learning.

### References